{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Programming Assignment 3: Information Retrieval\n",
    "\n",
    "In this assignment you will be improving upon a rather poorly-made information retrieval system. You will build an inverted index to quickly retrieve documents that match queries and then make it even better by using term-frequency inverse-document-frequency weighting and cosine similarity to compare queries to your data set. Your IR system will be evaluated for accuracy on the correct documents retrieved for different queries and the correctly computed tf-idf values and cosine similarities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "You will be using your IR system to find relevant documents among a collection of sixty short stories by Rider Haggard. The training data is located in the data/ directory under the subdirectory RiderHaggard/. Within this directory you will see yet another directory raw/. This contains the raw text files of the sixty short stories. The data/ directory also contains the files dev_queries.txt and dev_solutions.txt. We have provided these to you as a set of development queries and their expected answers to use as you begin implementing your IR system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Task\n",
    "\n",
    "Improve upon the given IR system by implementing the following features:\n",
    "\n",
    "<b>Inverted Positional Index:</b> Implement an inverted index - a mapping from words to the documents in which they occur, as well as the positions in the documents for which they occur.\n",
    "\n",
    "<b>Boolean Retrieval:</b> Implement a Boolean retrieval system, in which you return the list of documents that contain all words in a query. (Yes, you only need to support conjunctions for this assignment.)\n",
    "\n",
    "<b>Phrase Query Retrieval:</b> Implement a system that returns the list of documents in which the full phrase appears, (ie. the words of the query appear next to each other, in the specified order). Note that at the time of retrieval, you will not have access to the original documents anymore (the documents would be turned into bag of words), so you'll have to utilize your inverted positional index to complete this part.\n",
    "\n",
    "<b>TF-IDF:</b> Compute and store the term-frequency inverse-document-frequency value for every word-document co-occurrence:\n",
    "\n",
    "<b>Cosine Similarity:</b> Implement cosine similarity in order to improve upon the ranked retrieval system, which currently retrieves documents based upon the Jaccard coefficient between the query and each document. Also note that when computing $w_{t, q}$ (i.e. the weight for the word ùë§ in the query) do not include the idf term. That is, $w_{t, q} = 1 + \\log_{10} \\text{tf}_{t, q}$.\n",
    "<b> The reference solution uses ltc.lnn weighting for computing cosine scores. </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Your IR system will be evaluated on a development set of queries as well as a held-out set of queries. The queries are encoded in the file <b>dev_queries.txt</b> and are:\n",
    "\n",
    "- separation of church and state\n",
    "- white-robed priests\n",
    "- ancient underground city\n",
    "- native african queen\n",
    "- zulu king\n",
    "\n",
    "We test your system based on the five parts mentioned above: the inverted index (used both to get word positions and to get postings), boolean retrieval, phrase query retrieval, computing the correct tf-idf values, and implementing cosine similarity using the tf-idf values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Check\n",
    "\n",
    "Before we do anything else, let's quickly check that you're running the correct\n",
    "version of Python and are in the right environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "assert os.environ['CONDA_DEFAULT_ENV'] == \"cs124\"\n",
    "\n",
    "import sys\n",
    "assert sys.version_info.major == 3 and sys.version_info.minor == 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the above cell complains, it means that you're using the wrong environment\n",
    "or Python version!\n",
    "\n",
    "If so, please exit this notebook, kill the notebook server with CTRL-C, and\n",
    "try running\n",
    "\n",
    "$ conda activate cs124\n",
    "\n",
    "then restarting your notebook server with\n",
    "\n",
    "$ jupyter notebook\n",
    "\n",
    "If that doesn't work, you should go back and follow the installation\n",
    "instructions in PA0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started!\n",
    "\n",
    "We will first start by importing some modules and setting up our IR system class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import math\n",
    "\n",
    "from porter_stemmer import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IRSystem:\n",
    "    def __init__(self):\n",
    "        # For holding the data - initialized in read_data()\n",
    "        self.titles = []\n",
    "        self.docs = []\n",
    "        self.vocab = []\n",
    "        # For the text pre-processing.\n",
    "        self.alphanum = re.compile('[^a-zA-Z0-9]')\n",
    "        self.p = PorterStemmer()\n",
    "\n",
    "    def get_uniq_words(self):\n",
    "        uniq = set()\n",
    "        for doc in self.docs:\n",
    "            for word in doc:\n",
    "                uniq.add(word)\n",
    "        return uniq\n",
    "\n",
    "    def __read_raw_data(self, dirname):\n",
    "        print(\"Stemming Documents...\")\n",
    "\n",
    "        titles = []\n",
    "        docs = []\n",
    "        os.mkdir('%s/stemmed' % dirname)\n",
    "        title_pattern = re.compile('(.*) \\d+\\.txt')\n",
    "        # for debugging \n",
    "        #title_pattern = re.compile('(.*).txt')\n",
    "\n",
    "        # make sure we're only getting the files we actually want\n",
    "        filenames = []\n",
    "        for filename in os.listdir('%s/raw' % dirname):\n",
    "            if filename.endswith(\".txt\") and not filename.startswith(\".\"):\n",
    "                filenames.append(filename)\n",
    "                \n",
    "\n",
    "        for i, filename in enumerate(filenames):\n",
    "            title = title_pattern.search(filename).group(1)\n",
    "            print(\"    Doc %d of %d: %s\" % (i + 1, len(filenames), title))\n",
    "            titles.append(title)\n",
    "            contents = []\n",
    "            f = open('%s/raw/%s' % (dirname, filename), 'r', encoding=\"utf-8\")\n",
    "            of = open('%s/stemmed/%s.txt' % (dirname, title), 'w',\n",
    "                      encoding=\"utf-8\")\n",
    "            for line in f:\n",
    "                # make sure everything is lower case\n",
    "                line = line.lower()\n",
    "                # split on whitespace\n",
    "                line = [xx.strip() for xx in line.split()]\n",
    "                # remove non alphanumeric characters\n",
    "                line = [self.alphanum.sub('', xx) for xx in line]\n",
    "                # remove any words that are now empty\n",
    "                line = [xx for xx in line if xx != '']\n",
    "                # stem words\n",
    "                line = [self.p.stem(xx) for xx in line]\n",
    "                # add to the document's conents\n",
    "                contents.extend(line)\n",
    "                if len(line) > 0:\n",
    "                    of.write(\" \".join(line))\n",
    "                    of.write('\\n')\n",
    "            f.close()\n",
    "            of.close()\n",
    "            docs.append(contents)\n",
    "        return titles, docs\n",
    "\n",
    "    def __read_stemmed_data(self, dirname):\n",
    "        print(\"Already stemmed!\")\n",
    "        titles = []\n",
    "        docs = []\n",
    "\n",
    "        # make sure we're only getting the files we actually want\n",
    "        filenames = []\n",
    "        for filename in os.listdir('%s/stemmed' % dirname):\n",
    "            if filename.endswith(\".txt\") and not filename.startswith(\".\"):\n",
    "                filenames.append(filename)\n",
    "\n",
    "        \n",
    "        #comment for debugging purposes\n",
    "        if len(filenames) != 60:\n",
    "            msg = \"There are not 60 documents in ../data/RiderHaggard/stemmed/\\n\"\n",
    "            msg += \"Remove ../data/RiderHaggard/stemmed/ directory and re-run.\"\n",
    "            raise Exception(msg)\n",
    "        \n",
    "\n",
    "        for i, filename in enumerate(filenames):\n",
    "            title = filename.split('.')[0]\n",
    "            titles.append(title)\n",
    "            contents = []\n",
    "            f = open('%s/stemmed/%s' % (dirname, filename), 'r',\n",
    "                     encoding=\"utf-8\")\n",
    "            for line in f:\n",
    "                # split on whitespace\n",
    "                line = [xx.strip() for xx in line.split()]\n",
    "                # add to the document's conents\n",
    "                contents.extend(line)\n",
    "            f.close()\n",
    "            docs.append(contents)\n",
    "            \n",
    "        \n",
    "\n",
    "        return titles, docs\n",
    "\n",
    "    def read_data(self, dirname):\n",
    "        \"\"\"\n",
    "        Given the location of the 'data' directory, reads in the documents to\n",
    "        be indexed.\n",
    "        \"\"\"\n",
    "        # NOTE: We cache stemmed documents for speed\n",
    "        #       (i.e. write to files in new 'stemmed/' dir).\n",
    "\n",
    "        print(\"Reading in documents...\")\n",
    "        # dict mapping file names to list of \"words\" (tokens)\n",
    "        filenames = os.listdir(dirname)\n",
    "        subdirs = os.listdir(dirname)\n",
    "        if 'stemmed' in subdirs:\n",
    "            titles, docs = self.__read_stemmed_data(dirname)\n",
    "        else:\n",
    "            titles, docs = self.__read_raw_data(dirname)\n",
    "\n",
    "        # Sort document alphabetically by title to ensure we have the proper\n",
    "        # document indices when referring to them.\n",
    "        ordering = [idx for idx, title in sorted(enumerate(titles),\n",
    "                                                 key=lambda xx: xx[1])]\n",
    "\n",
    "        self.titles = []\n",
    "        self.docs = []\n",
    "        numdocs = len(docs)\n",
    "        for d in range(numdocs):\n",
    "            self.titles.append(titles[ordering[d]])\n",
    "            self.docs.append(docs[ordering[d]])\n",
    "\n",
    "        # Get the vocabulary.\n",
    "        self.vocab = [xx for xx in self.get_uniq_words()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing the inverted positional index logic \n",
    "\n",
    "document_1 = ['I', 'love', 'Mona']\n",
    "document_2 = ['Mona', 'is', 'so', 'cute']\n",
    "document_3 = ['Oh', 'Mona', 'Mona', 'so', 'cute']\n",
    "\n",
    "documents = [document_1, document_2, document_3]\n",
    "\n",
    "inv_index_test = {}\n",
    "\n",
    "for doc_id_test in range(len(documents)): \n",
    "    for position_test in range(len(documents[doc_id_test])): \n",
    "        word_test = documents[doc_id_test][position_test]\n",
    "        if word_test in inv_index_test.keys():\n",
    "            if doc_id_test in inv_index_test[word_test].keys():\n",
    "                inv_index_test[word_test][doc_id_test].append(position_test)\n",
    "            else: \n",
    "                inv_index_test[word_test][doc_id_test] = [position_test]\n",
    "        else: \n",
    "            inv_index_test[word_test] = {doc_id_test: [position_test]}\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'love', 'Mona']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'I': {0: [0]},\n",
       " 'love': {0: [1]},\n",
       " 'Mona': {0: [2], 1: [0], 2: [1, 2]},\n",
       " 'is': {1: [1]},\n",
       " 'so': {1: [2], 2: [3]},\n",
       " 'cute': {1: [3], 2: [4]},\n",
       " 'Oh': {2: [0]}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_index_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn documents into a map from ID to bag of words \n",
    "map_id_to_words = {}\n",
    "for d, doc in enumerate(documents): \n",
    "    bag_word = set(doc)\n",
    "    map_id_to_words[d] = bag_word \n",
    "    \n",
    "documents = map_id_to_words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will first build the inverted positional index (data structure that keeps track of the documents in which a particular word is contained, and the positions of that word in the document). The documents will have already been read in at this point. The following instance variables in the class are included in the starter code for you to use to build your inverted positional index: titles (a list of strings), docs (a list of lists of strings), and vocab (a list of strings)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'I', 'Mona', 'love'},\n",
       " 1: {'Mona', 'cute', 'is', 'so'},\n",
       " 2: {'Mona', 'Oh', 'cute', 'so'}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1_ID = list(documents.keys())[list(documents.values()).index(set(document_1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc1_ID\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_index_test['Mona'][doc1_ID]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(inv_index_test['Mona'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = ['Mona', 'Oh', 'cute']\n",
    "\n",
    "postings = []\n",
    "len_postings = []\n",
    "for word in query: \n",
    "    posting = list(inv_index_test[word].keys())\n",
    "    postings.append(posting)\n",
    "    len_postings.append(len(posting))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 2], [2], [1, 2]]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "postings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 1, 2]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_postings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "shorter_posting_index = len_postings.index(min(len_postings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shorter_posting_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shorter_posting = postings[shorter_posting_index]\n",
    "shorter_posting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 2]]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del postings[shorter_posting_index]\n",
    "postings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IRSystem(IRSystem):\n",
    "    def index(self):\n",
    "        \"\"\"\n",
    "        Build an index of the documents.\n",
    "        \"\"\"\n",
    "        print(\"Indexing...\")\n",
    "        # ------------------------------------------------------------------\n",
    "        # TODO: Create an inverted, positional index.\n",
    "        #       Granted this may not be a linked list as in a proper\n",
    "        #       implementation.\n",
    "        #       This index should allow easy access to both \n",
    "        #       1) the documents in which a particular word is contained, and \n",
    "        #       2) for every document, the positions of that word in the document \n",
    "        #       Some helpful instance variables:\n",
    "        #         * self.docs = List of documents\n",
    "        #         * self.titles = List of titles   => Why do we need self.titles ? \n",
    "\n",
    "        inv_index = {}\n",
    "        \n",
    "        '''\n",
    "        For inverted index : \n",
    "        For each term in each doc, get (term, docID) pairs. \n",
    "        Order the pairs alphabetically. \n",
    "        Merge in order to have (term, [docIDs])\n",
    "        For inverted positional index: \n",
    "        For each term, we need postings of the form docID: <position1, position2, ...> for all docIDs containing the term. \n",
    "        \n",
    "        inv_index = { term : {docID: [position1, position2...], docID: [position1, ...], ...}, term: dict of docs -> positions...} \n",
    "        \n",
    "        docID will be the index of the document in self.docs \n",
    "        \n",
    "        The inverted index will be a dictionnary, where the keys are the terms of the vocab, and the values are a dictionnary (where the keys are the docID of each doc containing the term, and the values is a list of the positions in the doc). \n",
    "        \n",
    "        '''\n",
    "\n",
    "        # Generate inverted index here\n",
    "        for doc_id in range(len(self.docs)): #for each doc, the doc_id is its position in the self.docs list \n",
    "            for position in range(len(self.docs[doc_id])): # for each word in the doc \n",
    "                word = self.docs[doc_id][position]\n",
    "                if word in inv_index.keys(): # if the word already exists in the dictionary \n",
    "                    #append its position to the list of positions inside the dict. \n",
    "                    if doc_id in inv_index[word].keys(): \n",
    "                        inv_index[word][doc_id].append(position)\n",
    "                    else: \n",
    "                        inv_index[word][doc_id] = [position]\n",
    "                else: \n",
    "                    inv_index[word] = {doc_id: [position]}\n",
    "\n",
    "        self.inv_index = inv_index\n",
    "\n",
    "        # ------------------------------------------------------------------\n",
    "\n",
    "        # turn self.docs into a map from ID to bag of words\n",
    "        id_to_bag_of_words = {}\n",
    "        for d, doc in enumerate(self.docs):\n",
    "            bag_of_words = set(doc)\n",
    "            id_to_bag_of_words[d] = bag_of_words\n",
    "        self.docs = id_to_bag_of_words\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict \n",
    "\n",
    "test_dict = defaultdict(dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict = defaultdict(lambda: defaultdict(list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict_2 = defaultdict(lambda: [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict_2[3].append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>()>, {3: [1]})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dict_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list, {})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dict[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dict['hello'][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will implement get_word_positions. This method returns a list of integers that identifies the positions in the document doc in which the word is found.  This is basically just an API into your inverted index, but you must implement it in order for the index to be evaluated fully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IRSystem(IRSystem):\n",
    "    def get_word_positions(self, word, doc):\n",
    "        \"\"\"\n",
    "        Given a word and a document, use the inverted index to return\n",
    "        \n",
    "        the positions of the specified word in the specified document.\n",
    "        \"\"\"\n",
    "        # ------------------------------------------------------------------\n",
    "        # TODO: return the list of positions for a word in a document.\n",
    "        positions = []\n",
    "               \n",
    "        #get the docID from self.docs\n",
    "        #docID = list(self.docs.keys())[list(self.docs.values()).index(set(doc))]\n",
    "        #for testing purposes \n",
    "\n",
    "        \n",
    "        # return inv_index[word][docID]\n",
    "        positions = self.inv_index[word][doc]\n",
    "\n",
    "        return positions\n",
    "        # ------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will add another method, get_posting, that returns a list of integers (document IDs) that identifies the documents in which the word is found. This is basically another API into your inverted index, but you must implement it in order to be evaluated fully.\n",
    "\n",
    "Keep in mind that the document IDs in each postings list to be sorted in order to perform the linear merge for boolean retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IRSystem(IRSystem):\n",
    "    def get_posting(self, word):\n",
    "        \"\"\"\n",
    "        Given a word, this returns the list of document indices (sorted) in\n",
    "        which the word occurs.\n",
    "        \"\"\"\n",
    "        # ------------------------------------------------------------------\n",
    "        # TODO: return the list of postings for a word.\n",
    "        posting = []\n",
    "        \n",
    "        posting = list(self.inv_index[word].keys())\n",
    "\n",
    "        return posting\n",
    "        # ------------------------------------------------------------------\n",
    "    \n",
    "    def get_posting_unstemmed(self, word):\n",
    "        \"\"\"\n",
    "        Given a word, this *stems* the word and then calls get_posting on the\n",
    "        stemmed word to get its postings list. You should *not* need to change\n",
    "        this function. It is needed for submission.\n",
    "        \"\"\"\n",
    "        word = self.p.stem(word)\n",
    "        return self.get_posting(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will implement Boolean retrieval, returning a list of document IDs corresponding to the documents in which all the words in query occur.\n",
    "\n",
    "Please implement the linear merge algorithm outlined in the videos/book (do not use built-in set intersection functions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IRSystem(IRSystem):\n",
    "    \n",
    "    def intersect(self, postings_1, postings_2):\n",
    "        i = 0\n",
    "        j = 0 \n",
    "        intersect = []\n",
    "        while i != len(postings_1) and j != len(postings_2):\n",
    "            if postings_1[i] == postings_2[j]:\n",
    "                intersect.append(postings_1[i])\n",
    "                i += 1\n",
    "                j += 1\n",
    "            else: \n",
    "                if postings_1[i] < postings_2[j]:\n",
    "                    i += 1\n",
    "                else: \n",
    "                    j += 1 \n",
    "        return intersect \n",
    "        \n",
    "    \n",
    "    def boolean_retrieve(self, query):\n",
    "        \"\"\"\n",
    "        Given a query in the form of a list of *stemmed* words, this returns\n",
    "        the list of documents in which *all* of those words occur (ie an AND\n",
    "        query).\n",
    "        Return an empty list if the query does not return any documents.\n",
    "        \"\"\"\n",
    "        # ------------------------------------------------------------------\n",
    "        # TODO: Implement Boolean retrieval. You will want to use your\n",
    "        #       inverted index that you created in index().\n",
    "        # Right now this just returns all the possible documents!\n",
    "        docs = []\n",
    "        postings = []\n",
    "        postings_length = []\n",
    "        \n",
    "        # first for each word in the query, get posting unstemmed \n",
    "        for word in query: \n",
    "            posting = self.get_posting_unstemmed(word)\n",
    "            postings.append(posting)\n",
    "            postings_length.append(len(posting))\n",
    "        \n",
    "        # get the intersection of all the postings in order of increasing document frequency (i.e. length of the postings)\n",
    "        while len(postings) > 1: \n",
    "            # get the shorter posting : create a helper function  \n",
    "            shorter_posting_index = postings_length.index(min(postings_length))\n",
    "            shorter_posting = postings[shorter_posting_index]\n",
    "            del postings_length[shorter_posting_index]\n",
    "            del postings[shorter_posting_index]\n",
    "            \n",
    "            #get the second shortest : same helper function probably \n",
    "            shorter_posting_index2 = postings_length.index(min(postings_length))\n",
    "            shorter_posting2 = postings[shorter_posting_index2]\n",
    "            del postings_length[shorter_posting_index2]\n",
    "            del postings[shorter_posting_index2]\n",
    "            \n",
    "            intersect = self.intersect(shorter_posting, shorter_posting2)\n",
    "            \n",
    "            # reinsert the intersect into the postings \n",
    "            if len(intersect) != 0: \n",
    "                postings.append(intersect)\n",
    "                postings_length.append(len(intersect))\n",
    "                \n",
    "            else: \n",
    "                break \n",
    "                \n",
    "        # the if is optionnal here right ? \n",
    "        if len(postings) == 1: \n",
    "            docs = postings[0]\n",
    "            \n",
    "        \n",
    "        \n",
    "        # ------------------------------------------------------------------\n",
    "\n",
    "        return sorted(docs)  # sorted doesn't actually matter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's continue to phase query retrieval. Our phrase_retrieve method will return a list of document IDs corresponding to the documents in which all the actual query phrase occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IRSystem(IRSystem):\n",
    "    \n",
    "    def intersect_k(self, positions_1, positions_2, k): \n",
    "        pos_1 = 0\n",
    "        pos_2 = 0\n",
    "        intersect = []\n",
    "        while pos_1 != len(positions_1) and pos_2 != len(positions_2): \n",
    "            if positions_2[pos_2] - positions_1[pos_1] == k:\n",
    "                intersect.append(positions_1[pos_1])\n",
    "                pos_1 += 1\n",
    "                pos_2 += 1\n",
    "            else: \n",
    "                if positions_2[pos_2] < positions_1[pos_1]:\n",
    "                    pos_2 += 1\n",
    "                else: \n",
    "                    pos_1 += 1\n",
    "        return intersect \n",
    "    \n",
    "    def phrase_retrieve(self, query):\n",
    "        \"\"\"\n",
    "        Given a query in the form of an ordered list of *stemmed* words, this \n",
    "        returns the list of documents in which *all* of those words occur, and \n",
    "        in the specified order. \n",
    "        Return an empty list if the query does not return any documents. \n",
    "        \"\"\"\n",
    "        # ------------------------------------------------------------------\n",
    "        # TODO: Implement Phrase Query retrieval (ie. return the documents \n",
    "        #       that don't just contain the words, but contain them in the \n",
    "        #       correct order) You will want to use the inverted index \n",
    "        #       that you created in index(), and may also consider using\n",
    "        #       boolean_retrieve. \n",
    "        #       NOTE that you no longer have access to the original documents\n",
    "        #       in self.docs because it is now a map from doc IDs to set\n",
    "        #       of unique words in the original document.\n",
    "        # Right now this just returns all possible documents!\n",
    "        docs = []\n",
    "        boolean_docs = self.boolean_retrieve(query)\n",
    "        result = []\n",
    "        \n",
    "        if len(query) <= 1: \n",
    "            docs = boolean_docs \n",
    "            \n",
    "        else: \n",
    "            for doc in boolean_docs: \n",
    "                positions = []\n",
    "                for word in query: \n",
    "                    positions.append(self.get_word_positions(word, doc))\n",
    "                #query has at least two words \n",
    "                result = self.intersect_k(positions[0], positions[1], 1)\n",
    "                i = 2\n",
    "                while i != len(query) and result != []:\n",
    "                    result = self.intersect_k(result, positions[i], i)\n",
    "                    i += 1\n",
    "                if result != []:\n",
    "                    docs.append(doc)\n",
    "                              \n",
    "        # ------------------------------------------------------------------\n",
    "\n",
    "        return sorted(docs)  # sorted doesn't actually matter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [['I', 'love', 'Mona', 'a', 'lot', 'I'], ['everybody', 'love', 'Mona'], ['Mona', 'and', 'I', 'love']]\n",
    "query = ['I', 'love', 'Mona']\n",
    "result = {}\n",
    "postings = [{0: [0], 2: [2]}, {0: [1], 1: [1]}, {0: [2], 1: [2], 2: [0]}]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersect_k_test(positions_1, positions_2, k): \n",
    "        pos_1 = 0\n",
    "        pos_2 = 0\n",
    "        intersect = []\n",
    "        while pos_1 != len(positions_1) and pos_2 != len(positions_2): \n",
    "            if positions_2[pos_2] - positions_1[pos_1] == k:\n",
    "                intersect.append(positions_1[pos_1])\n",
    "                pos_1 += 1\n",
    "                pos_2 += 1\n",
    "            else: \n",
    "                if positions_2[pos_2] < positions_1[pos_1]:\n",
    "                    pos_2 += 1\n",
    "                else: \n",
    "                    pos_1 += 1\n",
    "        return intersect "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "positions_test = [[0,5], [1], [2]]\n",
    "\n",
    "result_test = intersect_k_test(positions_test[0], positions_test[1], 1)\n",
    "\n",
    "result_test = intersect_k_test(result_test, positions_test[2], 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_test"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "result_test"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "result_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will compute and score the td-idf values. compute_tfidf and stores the tf-idf values for words and documents. For this you will probably want to be aware of the class variable vocab, which holds the list of all unique words, as well as the inverted index you created earlier.\n",
    "\n",
    "You must also implement get_tfidf to return the tf-idf weight for a particular word and document ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IRSystem(IRSystem):\n",
    "    def compute_tfidf(self):\n",
    "        # -------------------------------------------------------------------\n",
    "        # TODO: Compute and store TF-IDF values for words and documents in self.tfidf.\n",
    "        #       Recall that you can make use of:\n",
    "        #         * self.vocab: a list of all distinct (stemmed) words\n",
    "        #         * self.docs: a list of lists, where the i-th document is\n",
    "        #                   self.docs[i] => ['word1', 'word2', ..., 'wordN']\n",
    "        #       NOTE that you probably do *not* want to store a value for every\n",
    "        #       word-document pair, but rather just for those pairs where a\n",
    "        #       word actually occurs in the document.\n",
    "        print(\"Calculating tf-idf...\")\n",
    "        self.tfidf = {}\n",
    "        \n",
    "\n",
    "        # ------------------------------------------------------------------\n",
    "\n",
    "    def get_tfidf(self, word, document):\n",
    "        # ------------------------------------------------------------------\n",
    "        # TODO: Return the tf-idf weigthing for the given word (string) and\n",
    "        #       document index.\n",
    "        tfidf = 0.0\n",
    "        # ------------------------------------------------------------------\n",
    "        return tfidf\n",
    "\n",
    "    def get_tfidf_unstemmed(self, word, document):\n",
    "        \"\"\"\n",
    "        This function gets the TF-IDF of an *unstemmed* word in a document.\n",
    "        Stems the word and then calls get_tfidf. You should *not* need to\n",
    "        change this interface, but it is necessary for submission.\n",
    "        \"\"\"\n",
    "        word = self.p.stem(word)\n",
    "        return self.get_tfidf(word, document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we will implement rank_retrieve. This function returns a priority queue of the top ranked documents for a given query. Right now it ranks documents according to their Jaccard similarity with the query, but you will replace this method of ranking with a ranking using the <b>cosine similarity</b> between the documents and query.\n",
    "    \n",
    "Remember to use ltc.lnn weighting! This means that the query vector weights will be $1 + \\log_{10} \\text{tf}_{t, q}$ with no IDF term or normalization, and we only normalize by the length of the document vector (square root of the sum of squares of the tf-idf weights).\n",
    "    \n",
    "When we say normalize by \"document length\" or \"length of document\", we mean the length of the document vector, NOT the number of words in the actual text document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IRSystem(IRSystem):\n",
    "    def rank_retrieve(self, query):\n",
    "        \"\"\"\n",
    "        Given a query (a list of words), return a rank-ordered list of\n",
    "        documents (by ID) and score for the query.\n",
    "        \"\"\"\n",
    "        scores = [0.0 for xx in range(len(self.titles))]\n",
    "        # ------------------------------------------------------------------\n",
    "        # TODO: Implement cosine similarity between a document and a list of\n",
    "        #       query words.\n",
    "\n",
    "        # Right now, this code simply gets the score by taking the Jaccard\n",
    "        # similarity between the query and every document.\n",
    "        words_in_query = set()\n",
    "        for word in query:\n",
    "            words_in_query.add(word)\n",
    "\n",
    "        for d, words_in_doc in self.docs.items():\n",
    "            scores[d] = len(words_in_query.intersection(words_in_doc)) \\\n",
    "                        / float(len(words_in_query.union(words_in_doc)))\n",
    "\n",
    "        # ------------------------------------------------------------------\n",
    "\n",
    "        ranking = [idx for idx, sim in sorted(enumerate(scores),\n",
    "                                              key=lambda xx: xx[1],\n",
    "                                              reverse=True)]\n",
    "        results = []\n",
    "        for i in range(10):\n",
    "            results.append((ranking[i], scores[ranking[i]]))\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also add a few methods that given a string, will process and then return the list of matching documents for the different methods you have implemented. You do not need to add any additional code here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IRSystem(IRSystem): \n",
    "    def process_query(self, query_str):\n",
    "        \"\"\"\n",
    "        Given a query string, process it and return the list of lowercase,\n",
    "        alphanumeric, stemmed words in the string.\n",
    "        \"\"\"\n",
    "        # make sure everything is lower case\n",
    "        query = query_str.lower()\n",
    "        # split on whitespace\n",
    "        query = query.split()\n",
    "        # remove non alphanumeric characters\n",
    "        query = [self.alphanum.sub('', xx) for xx in query]\n",
    "        # stem words\n",
    "        query = [self.p.stem(xx) for xx in query]\n",
    "        return query\n",
    "\n",
    "    def query_retrieve(self, query_str):\n",
    "        \"\"\"\n",
    "        Given a string, process and then return the list of matching documents\n",
    "        found by boolean_retrieve().\n",
    "        \"\"\"\n",
    "        query = self.process_query(query_str)\n",
    "        return self.boolean_retrieve(query)\n",
    "\n",
    "    def phrase_query_retrieve(self, query_str):\n",
    "        \"\"\"\n",
    "        Given a string, process and then return the list of matching documents\n",
    "        found by phrase_retrieve().\n",
    "        \"\"\"\n",
    "        query = self.process_query(query_str)\n",
    "        return self.phrase_retrieve(query)\n",
    "\n",
    "    def query_rank(self, query_str):\n",
    "        \"\"\"\n",
    "        Given a string, process and then return the list of the top matching\n",
    "        documents, rank-ordered.\n",
    "        \"\"\"\n",
    "        query = self.process_query(query_str)\n",
    "        return self.rank_retrieve(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-95-5ec2da348313>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "set(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the code\n",
    "You can use the run_query function to test your code on a specific query. \n",
    "\n",
    "Note that the first time to run the run_query function, it will create a directory named stemmed/ in ../data/RiderHaggard/. This is meant to be a simple cache for the raw text documents. Later runs will be much faster after the first run. However, this means that if something happens during this first run and it does not get through processing all the documents, you may be left with an incomplete set of documents in ../data/RiderHaggard/stemmed/. If this happens, simply remove the stemmed/ directory and re-run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tests(irsys):\n",
    "    print(\"===== Running tests =====\")\n",
    "\n",
    "    ff = open('./data/dev_queries.txt')\n",
    "    questions = [xx.strip() for xx in ff.readlines()]\n",
    "    ff.close()\n",
    "    ff = open('./data/dev_solutions.txt')\n",
    "    solutions = [xx.strip() for xx in ff.readlines()]\n",
    "    ff.close()\n",
    "\n",
    "    epsilon = 1e-4\n",
    "    for part in range(6):\n",
    "        points = 0\n",
    "        num_correct = 0\n",
    "        num_total = 0\n",
    "\n",
    "        prob = questions[part]\n",
    "        soln = json.loads(solutions[part])\n",
    "\n",
    "        if part == 0:  # inverted index test\n",
    "            print(\"Inverted Index Test\")\n",
    "            queries = prob.split(\"; \")\n",
    "            queries = [xx.split(\", \") for xx in queries]\n",
    "            queries = [(xx[0], int(xx[1])) for xx in queries]\n",
    "            for i, (word, doc) in enumerate(queries):\n",
    "                num_total += 1\n",
    "                guess = irsys.get_word_positions(word, doc)\n",
    "                if sorted(guess) == soln[i]:\n",
    "                    num_correct += 1\n",
    "\n",
    "        if part == 1:  # get postings test\n",
    "            print(\"Get Postings Test\")\n",
    "            words = prob.split(\", \")\n",
    "            for i, word in enumerate(words):\n",
    "                num_total += 1\n",
    "                posting = irsys.get_posting_unstemmed(word)\n",
    "                if posting == soln[i]:\n",
    "                    num_correct += 1\n",
    "\n",
    "        elif part == 2:  # boolean retrieval test\n",
    "            print(\"Boolean Retrieval Test\")\n",
    "            queries = prob.split(\", \")\n",
    "            for i, query in enumerate(queries):\n",
    "                num_total += 1\n",
    "                guess = irsys.query_retrieve(query)\n",
    "                if set(guess) == set(soln[i]):\n",
    "                    num_correct += 1\n",
    "\n",
    "        elif part == 3:  # phrase query test\n",
    "            print(\"Phrase Query Retrieval\")\n",
    "            queries = prob.split(\", \")\n",
    "            for i, query in enumerate(queries):\n",
    "                num_total += 1\n",
    "                guess = irsys.phrase_query_retrieve(query)\n",
    "                if set(guess) == set(soln[i]):\n",
    "                    num_correct += 1\n",
    "\n",
    "        elif part == 4:  # tfidf test\n",
    "            print(\"TF-IDF Test\")\n",
    "            queries = prob.split(\"; \")\n",
    "            queries = [xx.split(\", \") for xx in queries]\n",
    "            queries = [(xx[0], int(xx[1])) for xx in queries]\n",
    "            for i, (word, doc) in enumerate(queries):\n",
    "                num_total += 1\n",
    "                guess = irsys.get_tfidf_unstemmed(word, doc)\n",
    "                if guess >= float(soln[i]) - epsilon and \\\n",
    "                        guess <= float(soln[i]) + epsilon:\n",
    "                    num_correct += 1\n",
    "\n",
    "        elif part == 5:  # cosine similarity test\n",
    "            print(\"Cosine Similarity Test\")\n",
    "            queries = prob.split(\", \")\n",
    "            for i, query in enumerate(queries):\n",
    "                num_total += 1\n",
    "                ranked = irsys.query_rank(query)\n",
    "                top_rank = ranked[0]\n",
    "                if top_rank[0] == soln[i][0]:\n",
    "                    if top_rank[1] >= float(soln[i][1]) - epsilon and \\\n",
    "                            top_rank[1] <= float(soln[i][1]) + epsilon:\n",
    "                        num_correct += 1\n",
    "\n",
    "        feedback = \"%d/%d Correct. Accuracy: %f\" % \\\n",
    "                   (num_correct, num_total, float(num_correct) / num_total)\n",
    "\n",
    "        if part == 1:\n",
    "            if num_correct == num_total:\n",
    "                points = 2\n",
    "            elif num_correct >= 0.5 * num_total:\n",
    "                points = 1\n",
    "            else:\n",
    "                points = 0\n",
    "        elif part == 2:\n",
    "            if num_correct == num_total:\n",
    "                points = 1\n",
    "            else:\n",
    "                points = 0\n",
    "        else:\n",
    "            if num_correct == num_total:\n",
    "                points = 3\n",
    "            elif num_correct > 0.75 * num_total:\n",
    "                points = 2\n",
    "            elif num_correct > 0:\n",
    "                points = 1\n",
    "            else:\n",
    "                points = 0\n",
    "\n",
    "        print(\"    Score: %d Feedback: %s\" % (points, feedback))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in documents...\n",
      "Already stemmed!\n",
      "Indexing...\n",
      "Calculating tf-idf...\n",
      "===== Running tests =====\n",
      "Inverted Index Test\n",
      "    Score: 3 Feedback: 5/5 Correct. Accuracy: 1.000000\n",
      "Get Postings Test\n",
      "    Score: 2 Feedback: 5/5 Correct. Accuracy: 1.000000\n",
      "Boolean Retrieval Test\n",
      "    Score: 1 Feedback: 5/5 Correct. Accuracy: 1.000000\n",
      "Phrase Query Retrieval\n",
      "    Score: 3 Feedback: 5/5 Correct. Accuracy: 1.000000\n",
      "TF-IDF Test\n",
      "    Score: 0 Feedback: 0/5 Correct. Accuracy: 0.000000\n",
      "Cosine Similarity Test\n",
      "    Score: 0 Feedback: 0/5 Correct. Accuracy: 0.000000\n"
     ]
    }
   ],
   "source": [
    "## Run this cell to run the tests\n",
    "irsys = IRSystem()\n",
    "irsys.read_data('./data/RiderHaggard')\n",
    "irsys.index()\n",
    "irsys.compute_tfidf()\n",
    "\n",
    "run_tests(irsys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in documents...\n",
      "Already stemmed!\n",
      "filenames\n",
      "Indexing...\n",
      "['titl', 'a', 'winter', 'pilgrimag', 'author', 'h', 'rider', 'haggard', 'first', 'publish', '1901', 'be', 'an', 'account', 'of', 'travel', 'through', 'palestin', 'itali', 'and', 'the', 'island', 'of', 'cypru', 'accomplish', 'in', 'the', 'year', '1900', 'dedic', 'i', 'offer', 'these', 'page', 'to', 'mr', 'mr', 'hart', 'bennett', 'and', 'all', 'other', 'cyprian', 'friend', 'whose', 'hospit', 'and', 'kind', 'have', 'made', 'my', 'sojourn', 'in', 'the', 'island', 'so', 'pleasant', 'to', 'rememb', 'ditchingham', '1901', 'a', 'winter', 'pilgrimag', 'chapter', 'i', 'milan', 'cathedr', 'sure', 'solomon', 'foresaw', 'these', 'dai', 'when', 'he', 'set', 'down', 'that', 'famou', 'sai', 'as', 'to', 'the', 'make', 'of', 'mani', 'book', 'the', 'aphor', 'i', 'confess', 'is', 'on', 'which', 'strike', 'me', 'through', 'with', 'shame', 'whenev', 'i', 'chanc', 'to', 'be', 'call', 'upon', 'to', 'read', 'it', 'aloud', 'in', 'the', 'parish', 'church', 'on', 'sundai', 'inde', 'it', 'suggest', 'to', 'me', 'a', 'tale', 'which', 'ha', 'a', 'moralor', 'a', 'parallel', 'some', 'month', 'ago', 'i', 'tarri', 'at', 'haifa', 'a', 'place', 'on', 'the', 'coast', 'of', 'syria', 'with', 'an', 'abomin', 'port', 'it', 'wa', 'at', 'or', 'about', 'the', 'hour', 'of', 'midnight', 'that', 'a', 'crowd', 'of', 'miser', 'travel', 'of', 'whom', 'i', 'wa', 'on', 'might', 'have', 'been', 'seen', 'cower', 'in', 'the', 'wind', 'and', 'rain', 'at', 'the', 'gate', 'of', 'thi', 'harbour', 'there', 'the', 'judg', 'and', 'the', 'offic', 'bulli', 'and', 'rent', 'them', 'caus', 'them', 'to', 'fumbl', 'with', 'damp', 'hand', 'and', 'discov', 'their', 'tezkereh', 'in', 'inaccess', 'pocket', 'which', 'thei', 'did', 'that', 'the', 'account', 'given', 'in', 'those', 'document', 'of', 'their', 'object', 'occup', 'past']\n",
      "['produc', 'by', 'john', 'bicker', 'dagni', 'emma', 'dud', 'david', 'widger', 'the', 'yellow', 'god', 'an', 'idol', 'of', 'africa', 'by', 'h', 'rider', 'haggard', 'chapter', 'i', 'sahara', 'limit', 'sir', 'robert', 'aylward', 'bart', 'mp', 'sat', 'in', 'hi', 'offic', 'in', 'the', 'citi', 'of', 'london', 'it', 'wa', 'a', 'veri', 'magnific', 'offic', 'quit', 'on', 'of', 'the', 'finest', 'that', 'could', 'be', 'found', 'within', 'half', 'a', 'mile', 'of', 'the', 'mansion', 'hous', 'it', 'exterior', 'wa', 'built', 'of', 'aberdeen', 'granit', 'a', 'materi', 'calcul', 'to', 'impress', 'the', 'prospect', 'investor', 'with', 'a', 'comfort', 'sens', 'of', 'secur', 'other', 'stucco', 'or', 'even', 'brickbuilt', 'offic', 'might', 'crumbl', 'and', 'fall', 'in', 'an', 'actual', 'or', 'a', 'financi', 'sens', 'but', 'thi', 'rocklik', 'edific', 'of', 'granit', 'surmount', 'by', 'a', 'lifes', 'statu', 'of', 'justic', 'with', 'her', 'scale', 'admir', 'from', 'either', 'corner', 'by', 'pleas', 'effigi', 'of', 'commerc', 'and', 'of', 'industri', 'would', 'sure', 'endur', 'ani', 'shock', 'earthquak', 'could', 'scarc', 'shake', 'it', 'strong', 'foundat', 'panic', 'and', 'disast', 'would', 'as', 'soon', 'affect', 'the', 'bank', 'of', 'england', 'that', 'at', 'least']\n",
      "Calculating tf-idf...\n"
     ]
    }
   ],
   "source": [
    "irsys_debug = IRSystem()\n",
    "irsys_debug.read_data('./data/RiderHaggardDebug')\n",
    "irsys_debug.index()\n",
    "irsys_debug.compute_tfidf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'titl': {0: [0]},\n",
       " 'a': {0: [1, 61, 121, 125, 127, 136, 157], 1: [40, 55, 68, 77, 96, 107]},\n",
       " 'winter': {0: [2, 62]},\n",
       " 'pilgrimag': {0: [3, 63]},\n",
       " 'author': {0: [4]},\n",
       " 'h': {0: [5], 1: [17]},\n",
       " 'rider': {0: [6], 1: [18]},\n",
       " 'haggard': {0: [7], 1: [19]},\n",
       " 'first': {0: [8]},\n",
       " 'publish': {0: [9]},\n",
       " '1901': {0: [10, 60]},\n",
       " 'be': {0: [11, 103], 1: [51]},\n",
       " 'an': {0: [12, 144], 1: [12, 93]},\n",
       " 'account': {0: [13, 212]},\n",
       " 'of': {0: [14, 22, 84, 141, 154, 159, 162, 180, 217],\n",
       "  1: [14, 36, 46, 57, 65, 80, 103, 110, 122, 125, 148]},\n",
       " 'travel': {0: [15, 161]},\n",
       " 'through': {0: [16, 96]},\n",
       " 'palestin': {0: [17]},\n",
       " 'itali': {0: [18]},\n",
       " 'and': {0: [19, 39, 46, 175, 186, 190, 200], 1: [90, 124, 140]},\n",
       " 'the': {0: [20, 26, 53, 82, 87, 111, 139, 152, 173, 178, 184, 187, 211],\n",
       "  1: [9, 34, 47, 58, 73, 146]},\n",
       " 'island': {0: [21, 54]},\n",
       " 'cypru': {0: [23]},\n",
       " 'accomplish': {0: [24]},\n",
       " 'in': {0: [25, 52, 110, 172, 204, 214], 1: [30, 33, 92]},\n",
       " 'year': {0: [27]},\n",
       " '1900': {0: [28]},\n",
       " 'dedic': {0: [29]},\n",
       " 'i': {0: [30, 65, 89, 100, 132, 164], 1: [21]},\n",
       " 'offer': {0: [31]},\n",
       " 'these': {0: [32, 71]},\n",
       " 'page': {0: [33]},\n",
       " 'to': {0: [34, 57, 81, 102, 106, 119, 195], 1: [71]},\n",
       " 'mr': {0: [35, 36]},\n",
       " 'hart': {0: [37]},\n",
       " 'bennett': {0: [38]},\n",
       " 'all': {0: [40]},\n",
       " 'other': {0: [41], 1: [82]},\n",
       " 'cyprian': {0: [42]},\n",
       " 'friend': {0: [43]},\n",
       " 'whose': {0: [44]},\n",
       " 'hospit': {0: [45]},\n",
       " 'kind': {0: [47]},\n",
       " 'have': {0: [48, 168]},\n",
       " 'made': {0: [49]},\n",
       " 'my': {0: [50]},\n",
       " 'sojourn': {0: [51]},\n",
       " 'so': {0: [55]},\n",
       " 'pleasant': {0: [56]},\n",
       " 'rememb': {0: [58]},\n",
       " 'ditchingham': {0: [59]},\n",
       " 'chapter': {0: [64], 1: [20]},\n",
       " 'milan': {0: [66]},\n",
       " 'cathedr': {0: [67]},\n",
       " 'sure': {0: [68], 1: [128]},\n",
       " 'solomon': {0: [69]},\n",
       " 'foresaw': {0: [70]},\n",
       " 'dai': {0: [72]},\n",
       " 'when': {0: [73]},\n",
       " 'he': {0: [74]},\n",
       " 'set': {0: [75]},\n",
       " 'down': {0: [76]},\n",
       " 'that': {0: [77, 156, 210], 1: [49, 150]},\n",
       " 'famou': {0: [78]},\n",
       " 'sai': {0: [79]},\n",
       " 'as': {0: [80], 1: [143]},\n",
       " 'make': {0: [83]},\n",
       " 'mani': {0: [85]},\n",
       " 'book': {0: [86]},\n",
       " 'aphor': {0: [88]},\n",
       " 'confess': {0: [90]},\n",
       " 'is': {0: [91]},\n",
       " 'on': {0: [92, 114, 138, 166], 1: [45]},\n",
       " 'which': {0: [93, 123, 207]},\n",
       " 'strike': {0: [94]},\n",
       " 'me': {0: [95, 120]},\n",
       " 'with': {0: [97, 143, 197], 1: [76, 112]},\n",
       " 'shame': {0: [98]},\n",
       " 'whenev': {0: [99]},\n",
       " 'chanc': {0: [101]},\n",
       " 'call': {0: [104]},\n",
       " 'upon': {0: [105]},\n",
       " 'read': {0: [107]},\n",
       " 'it': {0: [108, 117, 147], 1: [38, 61, 136]},\n",
       " 'aloud': {0: [109]},\n",
       " 'parish': {0: [112]},\n",
       " 'church': {0: [113]},\n",
       " 'sundai': {0: [115]},\n",
       " 'inde': {0: [116]},\n",
       " 'suggest': {0: [118]},\n",
       " 'tale': {0: [122]},\n",
       " 'ha': {0: [124]},\n",
       " 'moralor': {0: [126]},\n",
       " 'parallel': {0: [128]},\n",
       " 'some': {0: [129]},\n",
       " 'month': {0: [130]},\n",
       " 'ago': {0: [131]},\n",
       " 'tarri': {0: [133]},\n",
       " 'at': {0: [134, 149, 177], 1: [151]},\n",
       " 'haifa': {0: [135]},\n",
       " 'place': {0: [137]},\n",
       " 'coast': {0: [140]},\n",
       " 'syria': {0: [142]},\n",
       " 'abomin': {0: [145]},\n",
       " 'port': {0: [146]},\n",
       " 'wa': {0: [148, 165], 1: [39, 63]},\n",
       " 'or': {0: [150], 1: [84, 95]},\n",
       " 'about': {0: [151]},\n",
       " 'hour': {0: [153]},\n",
       " 'midnight': {0: [155]},\n",
       " 'crowd': {0: [158]},\n",
       " 'miser': {0: [160]},\n",
       " 'whom': {0: [163]},\n",
       " 'might': {0: [167], 1: [88]},\n",
       " 'been': {0: [169]},\n",
       " 'seen': {0: [170]},\n",
       " 'cower': {0: [171]},\n",
       " 'wind': {0: [174]},\n",
       " 'rain': {0: [176]},\n",
       " 'gate': {0: [179]},\n",
       " 'thi': {0: [181], 1: [100]},\n",
       " 'harbour': {0: [182]},\n",
       " 'there': {0: [183]},\n",
       " 'judg': {0: [185]},\n",
       " 'offic': {0: [188], 1: [32, 43, 87]},\n",
       " 'bulli': {0: [189]},\n",
       " 'rent': {0: [191]},\n",
       " 'them': {0: [192, 194]},\n",
       " 'caus': {0: [193]},\n",
       " 'fumbl': {0: [196]},\n",
       " 'damp': {0: [198]},\n",
       " 'hand': {0: [199]},\n",
       " 'discov': {0: [201]},\n",
       " 'their': {0: [202, 218]},\n",
       " 'tezkereh': {0: [203]},\n",
       " 'inaccess': {0: [205]},\n",
       " 'pocket': {0: [206]},\n",
       " 'thei': {0: [208]},\n",
       " 'did': {0: [209]},\n",
       " 'given': {0: [213]},\n",
       " 'those': {0: [215]},\n",
       " 'document': {0: [216]},\n",
       " 'object': {0: [219]},\n",
       " 'occup': {0: [220]},\n",
       " 'past': {0: [221]},\n",
       " 'produc': {1: [0]},\n",
       " 'by': {1: [1, 16, 106, 119]},\n",
       " 'john': {1: [2]},\n",
       " 'bicker': {1: [3]},\n",
       " 'dagni': {1: [4]},\n",
       " 'emma': {1: [5]},\n",
       " 'dud': {1: [6]},\n",
       " 'david': {1: [7]},\n",
       " 'widger': {1: [8]},\n",
       " 'yellow': {1: [10]},\n",
       " 'god': {1: [11]},\n",
       " 'idol': {1: [13]},\n",
       " 'africa': {1: [15]},\n",
       " 'sahara': {1: [22]},\n",
       " 'limit': {1: [23]},\n",
       " 'sir': {1: [24]},\n",
       " 'robert': {1: [25]},\n",
       " 'aylward': {1: [26]},\n",
       " 'bart': {1: [27]},\n",
       " 'mp': {1: [28]},\n",
       " 'sat': {1: [29]},\n",
       " 'hi': {1: [31]},\n",
       " 'citi': {1: [35]},\n",
       " 'london': {1: [37]},\n",
       " 'veri': {1: [41]},\n",
       " 'magnific': {1: [42]},\n",
       " 'quit': {1: [44]},\n",
       " 'finest': {1: [48]},\n",
       " 'could': {1: [50, 133]},\n",
       " 'found': {1: [52]},\n",
       " 'within': {1: [53]},\n",
       " 'half': {1: [54]},\n",
       " 'mile': {1: [56]},\n",
       " 'mansion': {1: [59]},\n",
       " 'hous': {1: [60]},\n",
       " 'exterior': {1: [62]},\n",
       " 'built': {1: [64]},\n",
       " 'aberdeen': {1: [66]},\n",
       " 'granit': {1: [67, 104]},\n",
       " 'materi': {1: [69]},\n",
       " 'calcul': {1: [70]},\n",
       " 'impress': {1: [72]},\n",
       " 'prospect': {1: [74]},\n",
       " 'investor': {1: [75]},\n",
       " 'comfort': {1: [78]},\n",
       " 'sens': {1: [79, 98]},\n",
       " 'secur': {1: [81]},\n",
       " 'stucco': {1: [83]},\n",
       " 'even': {1: [85]},\n",
       " 'brickbuilt': {1: [86]},\n",
       " 'crumbl': {1: [89]},\n",
       " 'fall': {1: [91]},\n",
       " 'actual': {1: [94]},\n",
       " 'financi': {1: [97]},\n",
       " 'but': {1: [99]},\n",
       " 'rocklik': {1: [101]},\n",
       " 'edific': {1: [102]},\n",
       " 'surmount': {1: [105]},\n",
       " 'lifes': {1: [108]},\n",
       " 'statu': {1: [109]},\n",
       " 'justic': {1: [111]},\n",
       " 'her': {1: [113]},\n",
       " 'scale': {1: [114]},\n",
       " 'admir': {1: [115]},\n",
       " 'from': {1: [116]},\n",
       " 'either': {1: [117]},\n",
       " 'corner': {1: [118]},\n",
       " 'pleas': {1: [120]},\n",
       " 'effigi': {1: [121]},\n",
       " 'commerc': {1: [123]},\n",
       " 'industri': {1: [126]},\n",
       " 'would': {1: [127, 142]},\n",
       " 'endur': {1: [129]},\n",
       " 'ani': {1: [130]},\n",
       " 'shock': {1: [131]},\n",
       " 'earthquak': {1: [132]},\n",
       " 'scarc': {1: [134]},\n",
       " 'shake': {1: [135]},\n",
       " 'strong': {1: [137]},\n",
       " 'foundat': {1: [138]},\n",
       " 'panic': {1: [139]},\n",
       " 'disast': {1: [141]},\n",
       " 'soon': {1: [144]},\n",
       " 'affect': {1: [145]},\n",
       " 'bank': {1: [147]},\n",
       " 'england': {1: [149]},\n",
       " 'least': {1: [152]}}"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "irsys_debug.inv_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Running tests =====\n",
      "Inverted Index Test\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'separ'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-178-e82c128f62aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_tests\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mirsys_debug\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-175-4c66e31c3bb6>\u001b[0m in \u001b[0;36mrun_tests\u001b[0;34m(irsys)\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0mnum_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0mguess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mirsys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_word_positions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mguess\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0msoln\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                     \u001b[0mnum_correct\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-168-acf86bb39ee0>\u001b[0m in \u001b[0;36mget_word_positions\u001b[0;34m(self, word, doc)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# return inv_index[word][docID]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mpositions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpositions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'separ'"
     ]
    }
   ],
   "source": [
    "run_tests(irsys_debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in documents...\n",
      "Already stemmed!\n",
      "Indexing...\n",
      "Calculating tf-idf...\n",
      "Best matching documents to 'My very own query':\n",
      "Long Odds: 2.240478e-03\n",
      "Hunter Quatermain's Story: 2.103787e-03\n",
      "The Tale of Three Lions: 1.574803e-03\n",
      "The Mahatma and the Hare: 1.283697e-03\n",
      "Black Heart and White Heart: 1.139818e-03\n",
      "Elissa: 1.117943e-03\n",
      "Moon of Israel: 1.059883e-03\n",
      "Morning Star: 9.930487e-04\n",
      "Maiwa's Revenge: 9.526834e-04\n",
      "Doctor Therne: 9.261403e-04\n"
     ]
    }
   ],
   "source": [
    "def run_query(query):\n",
    "    irsys = IRSystem()\n",
    "    irsys.read_data('./data/RiderHaggard')\n",
    "    irsys.index()\n",
    "    irsys.compute_tfidf()\n",
    "    \n",
    "    print(\"Best matching documents to '%s':\" % query)\n",
    "    results = irsys.query_rank(query)\n",
    "    for docId, score in results:\n",
    "        print(\"%s: %e\" % (irsys.titles[docId], score))\n",
    "        \n",
    "## Example query run where \"My very own query\" is your query.\n",
    "run_query(\"My very own query\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Retrieval Systems and Search Engines\n",
    "\n",
    "Safiya Umoja Noble‚Äôs <a href=\"https://nyupress.org/9781479837243/algorithms-of-oppression/\">Algorithms of Oppression</a> (2018) provides insight into how search engines and information retrieval algorithms can exhibit substantial racist and sexist biases. Noble demonstrates how prejudice against black women is embedded into search engine ranked results. These biases are apparent in both Google search‚Äôs autosuggestions and the first page of Google results. In this assignment, we have explored numerous features that are built into information retrieval systems, like Google Search. \n",
    "\n",
    "How could the algorithms you built in this assignment contribute to enforcing real-world biases? \n",
    "\n",
    "How can we reduce data discrimination and algorithmic bias that perpetuate gender and racial inequalities in search results and IR system?\n",
    "\n",
    "Please provide a short response to each of these questions (1-2 paragraphs per question)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IRSystem(IRSystem):\n",
    "    def algorithmic_bias_in_IR_systems(self):\n",
    "        # TODO: Place your response into the response string below\n",
    "        response = \"\"\n",
    "        return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you're ready to submit, you can run the cell below to prepare and zip\n",
    "up your solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "if [[ ! -f \"./pa3.ipynb\" ]]\n",
    "then\n",
    "    echo \"WARNING: Did not find notebook in Jupyter working directory. This probably means you're running on Google Colab. You'll need to go to File->Download .ipynb to download your notebok and other files, then zip them locally. See the README for more information.\"\n",
    "else\n",
    "    echo \"Found notebook file, creating submission zip...\"\n",
    "    zip -r submission.zip pa3.ipynb deps/\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're running on Google Colab, see the README for instructions on\n",
    "how to submit.\n",
    "\n",
    "__Best of luck!__\n",
    "\n",
    "__Some reminders for submission:__\n",
    "* If you have any extra files required for your implementation to work, make\n",
    " sure they are in a `deps/` folder on the same level as `pa3.ipynb` and\n",
    " include that folder in your submission zip file.\n",
    " * Make sure you didn't accidentally change the name of your notebook file,\n",
    " (it should be `pa3.ipynb`) as that is required for the autograder to work.\n",
    "* Go to Gradescope (gradescope.com), find the PA3 IR assignment and\n",
    "upload your zip file (`submission.zip`) as your solution.\n",
    "* Wait for the autograder to run (it should only take a minute or so) and check\n",
    "that your submission was graded successfully! If the autograder fails, or you\n",
    "get an unexpected score it may be a sign that your zip file was incorrect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
